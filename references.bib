
@misc{doshi-velez_towards_2017,
	title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1702.08608},
	doi = {10.48550/arXiv.1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	urldate = {2025-11-08},
	publisher = {arXiv},
	author = {Doshi-Velez, Finale and Kim, Been},
	month = mar,
	year = {2017},
	note = {arXiv:1702.08608 [stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{thapa_large_2025,
	title = {Large language models ({LLM}) in computational social science: prospects, current state, and challenges},
	volume = {15},
	issn = {1869-5469},
	shorttitle = {Large language models ({LLM}) in computational social science},
	url = {https://doi.org/10.1007/s13278-025-01428-9},
	doi = {10.1007/s13278-025-01428-9},
	abstract = {The advent of large language models (LLMs) has marked a new era in the transformation of computational social science (CSS). This paper dives into the role of LLMs in CSS, particularly exploring their potential to revolutionize data analysis and content generation and contribute to a broader understanding of social phenomena. We begin by discussing the applications of LLMs in various computational problems in social science including sentiment analysis, hate speech detection, stance and humor detection, misinformation detection, event understanding, and social network analysis, illustrating their capacity to generate nuanced insights into human behavior and societal trends. Furthermore, we explore the innovative use of LLMs in generating social media content. We also discuss the various ethical, technical, and legal issues these applications pose, and considerations required for responsible LLM usage. We further present the challenges associated with data bias, privacy, and the integration of these models into existing research frameworks. This paper aims to provide a solid background on the potential of LLMs in CSS, their past applications, current problems, and how they can pave the way for revolutionizing CSS.},
	language = {en},
	number = {1},
	urldate = {2025-08-12},
	journal = {Social Network Analysis and Mining},
	author = {Thapa, Surendrabikram and Shiwakoti, Shuvam and Shah, Siddhant Bikram and Adhikari, Surabhi and Veeramani, Hariram and Nasim, Mehwish and Naseem, Usman},
	month = mar,
	year = {2025},
	keywords = {Computational social science, Large language models, Natural language processing, Social network analysis, Social science},
	pages = {4},
}

@article{ullah_challenges_2024,
	title = {Challenges and barriers of using large language models ({LLM}) such as {ChatGPT} for diagnostic medicine with a focus on digital pathology – a recent scoping review},
	volume = {19},
	issn = {1746-1596},
	url = {https://doi.org/10.1186/s13000-024-01464-7},
	doi = {10.1186/s13000-024-01464-7},
	abstract = {The integration of large language models (LLMs) like ChatGPT in diagnostic medicine, with a focus on digital pathology, has garnered significant attention. However, understanding the challenges and barriers associated with the use of LLMs in this context is crucial for their successful implementation.},
	language = {en},
	number = {1},
	urldate = {2025-08-12},
	journal = {Diagnostic Pathology},
	author = {Ullah, Ehsan and Parwani, Anil and Baig, Mirza Mansoor and Singh, Rajendra},
	month = feb,
	year = {2024},
	keywords = {AI, Challenges and barriers of using LLMs, ChatGPT, Diagnostic medicine, Digital pathology, LLMs, Large learning models, ML, Pathology},
	pages = {43},
}

@inproceedings{kann_sentence-level_2018,
	address = {Brussels, Belgium},
	title = {Sentence-{Level} {Fluency} {Evaluation}: {References} {Help}, {But} {Can} {Be} {Spared}!},
	shorttitle = {Sentence-{Level} {Fluency} {Evaluation}},
	url = {https://aclanthology.org/K18-1031/},
	doi = {10.18653/v1/K18-1031},
	abstract = {Motivated by recent findings on the probabilistic modeling of acceptability judgments, we propose syntactic log-odds ratio (SLOR), a normalized language model score, as a metric for referenceless fluency evaluation of natural language generation output at the sentence level. We further introduce WPSLOR, a novel WordPiece-based version, which harnesses a more compact language model. Even though word-overlap metrics like ROUGE are computed with the help of hand-written references, our referenceless methods obtain a significantly higher correlation with human fluency scores on a benchmark dataset of compressed sentences. Finally, we present ROUGE-LM, a reference-based metric which is a natural extension of WPSLOR to the case of available references. We show that ROUGE-LM yields a significantly higher correlation with human judgments than all baseline metrics, including WPSLOR on its own.},
	urldate = {2025-08-11},
	booktitle = {Proceedings of the 22nd {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Kann, Katharina and Rothe, Sascha and Filippova, Katja},
	editor = {Korhonen, Anna and Titov, Ivan},
	month = oct,
	year = {2018},
	pages = {313--323},
}

@article{radford_language_nodate,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
}

@misc{treviso_crest_2023,
	title = {{CREST}: {A} {Joint} {Framework} for {Rationalization} and {Counterfactual} {Text} {Generation}},
	shorttitle = {{CREST}},
	url = {http://arxiv.org/abs/2305.17075},
	doi = {10.48550/arXiv.2305.17075},
	abstract = {Selective rationales and counterfactual examples have emerged as two effective, complementary classes of interpretability methods for analyzing and training NLP models. However, prior work has not explored how these methods can be integrated to combine their complementary advantages. We overcome this limitation by introducing CREST (ContRastive Edits with Sparse raTionalization), a joint framework for selective rationalization and counterfactual text generation, and show that this framework leads to improvements in counterfactual quality, model robustness, and interpretability. First, CREST generates valid counterfactuals that are more natural than those produced by previous methods, and subsequently can be used for data augmentation at scale, reducing the need for human-generated examples. Second, we introduce a new loss function that leverages CREST counterfactuals to regularize selective rationales and show that this regularization improves both model robustness and rationale quality, compared to methods that do not leverage CREST counterfactuals. Our results demonstrate that CREST successfully bridges the gap between selective rationales and counterfactual examples, addressing the limitations of existing methods and providing a more comprehensive view of a model's predictions.},
	urldate = {2025-08-11},
	publisher = {arXiv},
	author = {Treviso, Marcos and Ross, Alexis and Guerreiro, Nuno M. and Martins, André F. T.},
	month = may,
	year = {2023},
	note = {arXiv:2305.17075 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{yang_generating_2020,
	title = {Generating {Plausible} {Counterfactual} {Explanations} for {Deep} {Transformers} in {Financial} {Text} {Classification}},
	url = {http://arxiv.org/abs/2010.12512},
	doi = {10.48550/arXiv.2010.12512},
	abstract = {Corporate mergers and acquisitions (M\&A) account for billions of dollars of investment globally every year, and offer an interesting and challenging domain for artificial intelligence. However, in these highly sensitive domains, it is crucial to not only have a highly robust and accurate model, but be able to generate useful explanations to garner a user's trust in the automated system. Regrettably, the recent research regarding eXplainable AI (XAI) in financial text classification has received little to no attention, and many current methods for generating textual-based explanations result in highly implausible explanations, which damage a user's trust in the system. To address these issues, this paper proposes a novel methodology for producing plausible counterfactual explanations, whilst exploring the regularization benefits of adversarial training on language models in the domain of FinTech. Exhaustive quantitative experiments demonstrate that not only does this approach improve the model accuracy when compared to the current state-of-the-art and human performance, but it also generates counterfactual explanations which are significantly more plausible based on human trials.},
	urldate = {2025-08-11},
	publisher = {arXiv},
	author = {Yang, Linyi and Kenny, Eoin M. and Ng, Tin Lok James and Yang, Yi and Smyth, Barry and Dong, Ruihai},
	month = oct,
	year = {2020},
	note = {arXiv:2010.12512 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@incollection{ancona_gradient-based_2019,
	address = {Cham},
	title = {Gradient-{Based} {Attribution} {Methods}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_9},
	abstract = {The problem of explaining complex machine learning models, including Deep Neural Networks, has gained increasing attention over the last few years. While several methods have been proposed to explain network predictions, the definition itself of explanation is still debated. Moreover, only a few attempts to compare explanation methods from a theoretical perspective has been done. In this chapter, we discuss the theoretical properties of several attribution methods and show how they share the same idea of using the gradient information as a descriptive factor for the functioning of a model. Finally, we discuss the strengths and limitations of these methods and compare them with available alternatives.},
	language = {en},
	urldate = {2025-08-11},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_9},
	keywords = {Attribution methods, Deep Neural Networks, Explainable artificial intelligence},
	pages = {169--191},
}

@incollection{samek_gradient-based_2019,
	address = {Cham},
	title = {Gradient-{Based} {Attribution} {Methods}},
	volume = {11700},
	isbn = {978-3-030-28953-9 978-3-030-28954-6},
	url = {http://link.springer.com/10.1007/978-3-030-28954-6_9},
	abstract = {The problem of explaining complex machine learning models, including Deep Neural Networks, has gained increasing attention over the last few years. While several methods have been proposed to explain network predictions, the deﬁnition itself of explanation is still debated. Moreover, only a few attempts to compare explanation methods from a theoretical perspective has been done. In this chapter, we discuss the theoretical properties of several attribution methods and show how they share the same idea of using the gradient information as a descriptive factor for the functioning of a model. Finally, we discuss the strengths and limitations of these methods and compare them with available alternatives.},
	language = {en},
	urldate = {2025-08-11},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {169--191},
}

@incollection{samek_gradient-based_2019-1,
	address = {Cham},
	title = {Gradient-{Based} {Attribution} {Methods}},
	volume = {11700},
	isbn = {978-3-030-28953-9 978-3-030-28954-6},
	url = {http://link.springer.com/10.1007/978-3-030-28954-6_9},
	abstract = {The problem of explaining complex machine learning models, including Deep Neural Networks, has gained increasing attention over the last few years. While several methods have been proposed to explain network predictions, the deﬁnition itself of explanation is still debated. Moreover, only a few attempts to compare explanation methods from a theoretical perspective has been done. In this chapter, we discuss the theoretical properties of several attribution methods and show how they share the same idea of using the gradient information as a descriptive factor for the functioning of a model. Finally, we discuss the strengths and limitations of these methods and compare them with available alternatives.},
	language = {en},
	urldate = {2025-08-11},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {169--191},
}

@misc{sundararajan_gradients_2016,
	title = {Gradients of {Counterfactuals}},
	url = {http://arxiv.org/abs/1611.02639},
	doi = {10.48550/arXiv.1611.02639},
	abstract = {Gradients have been used to quantify feature importance in machine learning models. Unfortunately, in nonlinear deep networks, not only individual neurons but also the whole network can saturate, and as a result an important input feature can have a tiny gradient. We study various networks, and observe that this phenomena is indeed widespread, across many inputs. We propose to examine interior gradients, which are gradients of counterfactual inputs constructed by scaling down the original input. We apply our method to the GoogleNet architecture for object recognition in images, as well as a ligand-based virtual screening network with categorical features and an LSTM based language model for the Penn Treebank dataset. We visualize how interior gradients better capture feature importance. Furthermore, interior gradients are applicable to a wide variety of deep networks, and have the attribution property that the feature importance scores sum to the the prediction score. Best of all, interior gradients can be computed just as easily as gradients. In contrast, previous methods are complex to implement, which hinders practical adoption.},
	urldate = {2025-08-11},
	publisher = {arXiv},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	month = nov,
	year = {2016},
	note = {arXiv:1611.02639 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@incollection{gianfagna_model-agnostic_2021,
	address = {Cham},
	title = {Model-{Agnostic} {Methods} for {XAI}},
	isbn = {978-3-030-68640-6},
	url = {https://doi.org/10.1007/978-3-030-68640-6_4},
	abstract = {In this chapter, we start our journey through XAI model-agnostic methods that are, as we said, potent techniques to produce explanations without relying on ML model internals that are “opaque.”},
	language = {en},
	urldate = {2025-08-11},
	booktitle = {Explainable {AI} with {Python}},
	publisher = {Springer International Publishing},
	author = {Gianfagna, Leonida and Di Cecco, Antonio},
	editor = {Gianfagna, Leonida and Di Cecco, Antonio},
	year = {2021},
	doi = {10.1007/978-3-030-68640-6_4},
	pages = {81--113},
}

@misc{raffel_exploring_2023,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	doi = {10.48550/arXiv.1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2025-08-09},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = sep,
	year = {2023},
	note = {arXiv:1910.10683 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wu_polyjuice_2021,
	title = {Polyjuice: {Generating} {Counterfactuals} for {Explaining}, {Evaluating}, and {Improving} {Models}},
	shorttitle = {Polyjuice},
	url = {http://arxiv.org/abs/2101.00288},
	doi = {10.48550/arXiv.2101.00288},
	abstract = {While counterfactual examples are useful for analysis and training of NLP models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as paraphrases or word substitutions. We present Polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences. We show that Polyjuice produces diverse sets of realistic counterfactuals, which in turn are useful in various distinct applications: improving training and evaluation on three different tasks (with around 70\% less annotation effort than manual generation), augmenting state-of-the-art explanation techniques, and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts.},
	urldate = {2025-08-09},
	publisher = {arXiv},
	author = {Wu, Tongshuang and Ribeiro, Marco Tulio and Heer, Jeffrey and Weld, Daniel S.},
	month = jun,
	year = {2021},
	note = {arXiv:2101.00288 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{madaan_generate_2021,
	title = {Generate {Your} {Counterfactuals}: {Towards} {Controlled} {Counterfactual} {Generation} for {Text}},
	shorttitle = {Generate {Your} {Counterfactuals}},
	url = {http://arxiv.org/abs/2012.04698},
	doi = {10.48550/arXiv.2012.04698},
	abstract = {Machine Learning has seen tremendous growth recently, which has led to larger adoption of ML systems for educational assessments, credit risk, healthcare, employment, criminal justice, to name a few. The trustworthiness of ML and NLP systems is a crucial aspect and requires a guarantee that the decisions they make are fair and robust. Aligned with this, we propose a framework GYC, to generate a set of counterfactual text samples, which are crucial for testing these ML systems. Our main contributions include a) We introduce GYC, a framework to generate counterfactual samples such that the generation is plausible, diverse, goal-oriented, and effective, b) We generate counterfactual samples, that can direct the generation towards a corresponding condition such as named-entity tag, semantic role label, or sentiment. Our experimental results on various domains show that GYC generates counterfactual text samples exhibiting the above four properties. GYC generates counterfactuals that can act as test cases to evaluate a model and any text debiasing algorithm.},
	urldate = {2025-08-09},
	publisher = {arXiv},
	author = {Madaan, Nishtha and Padhi, Inkit and Panwar, Naveen and Saha, Diptikalyan},
	month = mar,
	year = {2021},
	note = {arXiv:2012.04698 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{yang_generating_2020-1,
	title = {Generating {Plausible} {Counterfactual} {Explanations} for {Deep} {Transformers} in {Financial} {Text} {Classification}},
	url = {http://arxiv.org/abs/2010.12512},
	doi = {10.48550/arXiv.2010.12512},
	abstract = {Corporate mergers and acquisitions (M\&A) account for billions of dollars of investment globally every year, and offer an interesting and challenging domain for artificial intelligence. However, in these highly sensitive domains, it is crucial to not only have a highly robust and accurate model, but be able to generate useful explanations to garner a user's trust in the automated system. Regrettably, the recent research regarding eXplainable AI (XAI) in financial text classification has received little to no attention, and many current methods for generating textual-based explanations result in highly implausible explanations, which damage a user's trust in the system. To address these issues, this paper proposes a novel methodology for producing plausible counterfactual explanations, whilst exploring the regularization benefits of adversarial training on language models in the domain of FinTech. Exhaustive quantitative experiments demonstrate that not only does this approach improve the model accuracy when compared to the current state-of-the-art and human performance, but it also generates counterfactual explanations which are significantly more plausible based on human trials.},
	urldate = {2025-08-09},
	publisher = {arXiv},
	author = {Yang, Linyi and Kenny, Eoin M. and Ng, Tin Lok James and Yang, Yi and Smyth, Barry and Dong, Ruihai},
	month = oct,
	year = {2020},
	note = {arXiv:2010.12512 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{li_linguistically-informed_2020,
	address = {Online},
	title = {Linguistically-{Informed} {Transformations} ({LIT}): {A} {Method} for {Automatically} {Generating} {Contrast} {Sets}},
	shorttitle = {Linguistically-{Informed} {Transformations} ({LIT})},
	url = {https://aclanthology.org/2020.blackboxnlp-1.12/},
	doi = {10.18653/v1/2020.blackboxnlp-1.12},
	abstract = {Although large-scale pretrained language models, such as BERT and RoBERTa, have achieved superhuman performance on in-distribution test sets, their performance suffers on out-of-distribution test sets (e.g., on contrast sets). Building contrast sets often requires human-expert annotation, which is expensive and hard to create on a large scale. In this work, we propose a Linguistically-Informed Transformation (LIT) method to automatically generate contrast sets, which enables practitioners to explore linguistic phenomena of interests as well as compose different phenomena. Experimenting with our method on SNLI and MNLI shows that current pretrained language models, although being claimed to contain sufficient linguistic knowledge, struggle on our automatically generated contrast sets. Furthermore, we improve models' performance on the contrast sets by applying LIT to augment the training data, without affecting performance on the original data.},
	urldate = {2025-08-09},
	booktitle = {Proceedings of the {Third} {BlackboxNLP} {Workshop} on {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Chuanrong and Shengshuo, Lin and Liu, Zeyu and Wu, Xinyi and Zhou, Xuhui and Steinert-Threlkeld, Shane},
	editor = {Alishahi, Afra and Belinkov, Yonatan and Chrupała, Grzegorz and Hupkes, Dieuwke and Pinter, Yuval and Sajjad, Hassan},
	month = nov,
	year = {2020},
	pages = {126--135},
}

@misc{dandl_multi-objective_2020,
	title = {Multi-{Objective} {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/2004.11165},
	doi = {10.48550/arXiv.2004.11165},
	abstract = {Counterfactual explanations are one of the most popular methods to make predictions of black box machine learning models interpretable by providing explanations in the form of `what-if scenarios'. Most current approaches optimize a collapsed, weighted sum of multiple objectives, which are naturally difficult to balance a-priori. We propose the Multi-Objective Counterfactuals (MOC) method, which translates the counterfactual search into a multi-objective optimization problem. Our approach not only returns a diverse set of counterfactuals with different trade-offs between the proposed objectives, but also maintains diversity in feature space. This enables a more detailed post-hoc analysis to facilitate better understanding and also more options for actionable user responses to change the predicted outcome. Our approach is also model-agnostic and works for numerical and categorical input features. We show the usefulness of MOC in concrete cases and compare our approach with state-of-the-art methods for counterfactual explanations.},
	urldate = {2025-08-09},
	publisher = {arXiv},
	author = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
	month = jun,
	year = {2020},
	note = {arXiv:2004.11165},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wachter_counterfactual_2018,
	title = {Counterfactual {Explanations} without {Opening} the {Black} {Box}: {Automated} {Decisions} and the {GDPR}},
	shorttitle = {Counterfactual {Explanations} without {Opening} the {Black} {Box}},
	url = {http://arxiv.org/abs/1711.00399},
	doi = {10.48550/arXiv.1711.00399},
	abstract = {There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
	urldate = {2025-08-09},
	publisher = {arXiv},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	month = mar,
	year = {2018},
	note = {arXiv:1711.00399},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{miller_explanation_2019,
	title = {Explanation in artificial intelligence: {Insights} from the social sciences},
	volume = {267},
	issn = {0004-3702},
	shorttitle = {Explanation in artificial intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
	doi = {10.1016/j.artint.2018.07.007},
	abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
	urldate = {2025-08-09},
	journal = {Artificial Intelligence},
	author = {Miller, Tim},
	month = feb,
	year = {2019},
	keywords = {Explainability, Explainable AI, Explanation, Interpretability, Transparency},
	pages = {1--38},
}

@misc{shliazhko_mgpt_2023,
	title = {{mGPT}: {Few}-{Shot} {Learners} {Go} {Multilingual}},
	shorttitle = {{mGPT}},
	url = {http://arxiv.org/abs/2204.07580},
	doi = {10.48550/arXiv.2204.07580},
	abstract = {Recent studies report that autoregressive language models can successfully solve many NLP tasks via zero- and few-shot learning paradigms, which opens up new possibilities for using the pre-trained language models. This paper introduces two autoregressive GPT-like models with 1.3 billion and 13 billion parameters trained on 60 languages from 25 language families using Wikipedia and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron frameworks allow us to parallelize the training and inference steps effectively. The resulting models show performance on par with the recently released XGLM models by Facebook, covering more languages and enhancing NLP possibilities for low resource languages of CIS countries and Russian small nations. We detail the motivation for the choices of the architecture design, thoroughly describe the data preparation pipeline, and train five small versions of the model to choose the most optimal multilingual tokenization strategy. We measure the model perplexity in all covered languages and evaluate it on the wide spectre of multilingual tasks, including classification, generative, sequence labeling and knowledge probing. The models were evaluated with the zero-shot and few-shot methods. Furthermore, we compared the classification tasks with the state-of-the-art multilingual model XGLM. source code and the mGPT XL model are publicly released.},
	urldate = {2025-07-29},
	publisher = {arXiv},
	author = {Shliazhko, Oleh and Fenogenova, Alena and Tikhonova, Maria and Mikhailov, Vladislav and Kozlova, Anastasia and Shavrina, Tatiana},
	month = oct,
	year = {2023},
	note = {arXiv:2204.07580},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2025-07-28},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{maas_learning_2011,
	address = {Portland, Oregon, USA},
	title = {Learning {Word} {Vectors} for {Sentiment} {Analysis}},
	url = {https://aclanthology.org/P11-1015/},
	urldate = {2025-07-28},
	booktitle = {Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
	editor = {Lin, Dekang and Matsumoto, Yuji and Mihalcea, Rada},
	month = jun,
	year = {2011},
	pages = {142--150},
}

@misc{benoit_cea_chilean_2025,
	title = {Chilean {Twitter} {Hate} {Speech} {Dataset}: {CL2}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Chilean {Twitter} {Hate} {Speech} {Dataset}},
	url = {https://zenodo.org/doi/10.5281/zenodo.14619077},
	doi = {10.5281/ZENODO.14619077},
	abstract = {In the last few years, several organizations have manifested their concern over the increase in use of Hateful Speech or Hate Speech for short, this concept refers to forms of expression or audiovisual content that encourage discrimination or violence against individuals or groups solely based on their gender, sexual orientation, ethnicity, religion, or nationality. Being able to monitor this phenomenon in a timely manner can help societies and their governments to prevent tensions, crimes, and conflicts that endangers not only the most fundamental democratic values but also order stability and social peace.

 

The fast massification of social platforms has transformed them into one of the main mediums used by people today for creating and sharing information. Consequently, social media platforms such as Twitter, Instagram, or Facebook are the staging in which Hate Speech is mostly propagated today. Sadly the great reach of these platforms, their public nature, the social dynamics that are perpetuated in them and the absence of an explicit regulatory framework, only worsen and increase the magnitude of this phenomena. Mining such conversations, such as Tweets, to develop a dataset can serve as a data resource for interdisciplinary research related to the analysis of interest, views, opinions and help us in the creation of tools to further our understanding of social dynamics related to Hate Speech propagation and analysis.},
	language = {es},
	urldate = {2025-07-28},
	publisher = {Zenodo},
	author = {Benoit Cea, Domingo and Ñanculef, Ricardo and Mendoza, Marcelo},
	month = jan,
	year = {2025},
}

@inproceedings{godoy_automatic_2024,
	address = {London, United Kingdom},
	title = {Automatic detection of contextual laterality in {Mammography} {Reports} using {Large} {Language} {Models}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350375657},
	url = {https://ieeexplore.ieee.org/document/10677842/},
	doi = {10.1109/ICPRS62101.2024.10677842},
	urldate = {2025-07-28},
	booktitle = {2024 14th {International} {Conference} on {Pattern} {Recognition} {Systems} ({ICPRS})},
	publisher = {IEEE},
	author = {Godoy, Eduardo and De Ferrari, Joaquín and Mellado, Diego and Chabert, Steren and Salas, Rodrigo},
	month = jul,
	year = {2024},
	pages = {1--6},
}

@article{baron_explainable_2023,
	title = {Explainable {AI} and {Causal} {Understanding}: {Counterfactual} {Approaches} {Considered}},
	volume = {33},
	issn = {1572-8641},
	url = {https://doi.org/10.1007/s11023-023-09637-x},
	doi = {10.1007/s11023-023-09637-x},
	abstract = {The counterfactual approach to explainable AI (XAI) seeks to provide understanding of AI systems through the provision of counterfactual explanations. In a recent systematic review, Chou et al. (Inform Fus 81:59–83, 2022) argue that the counterfactual approach does not clearly provide causal understanding. They diagnose the problem in terms of the underlying framework within which the counterfactual approach has been developed. To date, the counterfactual approach has not been developed in concert with the approach for specifying causes developed by Pearl (Causality: Models, reasoning, and inference. Cambridge University Press, 2000) and Woodward (Making things happen: A theory of causal explanation. Oxford University Press, 2003). In this paper, I build on Chou et al.’s work by applying the Pearl-Woodward approach. I argue that the standard counterfactual approach to XAI is capable of delivering causal understanding, but that there are limitations on its capacity to do so. I suggest a way to overcome these limitations.},
	number = {2},
	journal = {Minds and Machines},
	author = {Baron, Sam},
	month = jun,
	year = {2023},
	pages = {347--377},
}

@inproceedings{mittal_enhancing_2023,
	title = {Enhancing {Hate} {Speech} {Detection} through {Explainable} {AI}},
	url = {https://ieeexplore.ieee.org/document/10127897},
	doi = {10.1109/ICSMDI57622.2023.00028},
	abstract = {The potential of XAI in detecting hate speech using deep learning models is versatile and multifaceted. To better understand the decision-making process of complex AI models, this study applied XAI to the dataset and investigated the interpretability and explanation of their decisions. The data was preprocessed by cleaning, tokenizing, lemmatizing, and removing inconsistencies in tweets. Simplification of categorical variables was also performed during training. Exploratory data analysis was conducted to identify patterns and insights in the dataset. The study used a set of existing models, including LIME, SHAP, XGBoost, and KTrain, to analyze the accuracy. The KTrain model achieved the highest accuracy and lowest loss among the variants developed to increase explainability.},
	urldate = {2024-09-12},
	booktitle = {2023 3rd {International} {Conference} on {Smart} {Data} {Intelligence} ({ICSMDI})},
	author = {Mittal, Dipti and Singh, Harmeet},
	month = mar,
	year = {2023},
	keywords = {Cleaning, Data analysis, Data models, Decision making, Deep learning, Hate speech, Knowledge Transfer for Deep Learning and Explainable Artificial Intelligence, Local Interpretable Model-Agnostic Explanations, SHapley Additive exPlanations, Training, hate speech, offensive languages},
	pages = {118--123},
}

@inproceedings{kocmi_findings_2023,
	address = {Singapore},
	title = {Findings of the 2023 {Conference} on {Machine} {Translation} ({WMT23}): {LLMs} {Are} {Here} but {Not} {Quite} {There} {Yet}},
	shorttitle = {Findings of the 2023 {Conference} on {Machine} {Translation} ({WMT23})},
	url = {https://aclanthology.org/2023.wmt-1.1},
	doi = {10.18653/v1/2023.wmt-1.1},
	abstract = {This paper presents the results of the General Machine Translation Task organised as part of the 2023 Conference on Machine Translation (WMT). In the general MT task, participants were asked to build machine translation systems for any of 8 language pairs (corresponding to 14 translation directions), to be evaluated on test sets consisting of up to four different domains. We evaluate system outputs with professional human annotators using a combination of source-based Direct Assessment and scalar quality metric (DA+SQM).},
	urldate = {2024-09-03},
	booktitle = {Proceedings of the {Eighth} {Conference} on {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Kocmi, Tom and Avramidis, Eleftherios and Bawden, Rachel and Bojar, Ondřej and Dvorkovich, Anton and Federmann, Christian and Fishel, Mark and Freitag, Markus and Gowda, Thamme and Grundkiewicz, Roman and Haddow, Barry and Koehn, Philipp and Marie, Benjamin and Monz, Christof and Morishita, Makoto and Murray, Kenton and Nagata, Makoto and Nakazawa, Toshiaki and Popel, Martin and Popović, Maja and Shmatova, Mariya},
	editor = {Koehn, Philipp and Haddow, Barry and Kocmi, Tom and Monz, Christof},
	month = dec,
	year = {2023},
	pages = {1--42},
}

@article{jelinek_perplexitymeasure_1977,
	title = {Perplexity—a measure of the difficulty of speech recognition tasks},
	volume = {62},
	issn = {0001-4966, 1520-8524},
	url = {https://pubs.aip.org/jasa/article/62/S1/S63/642598/Perplexity-a-measure-of-the-difficulty-of-speech},
	doi = {10.1121/1.2016299},
	abstract = {Using counterexamples, we show that vocabulary size and static and dynamic branching factors are all inadequate as measures of speech recognition complexity of finite state grammars. Information theoretic arguments show that perplexity (the logarithm of which is the familiar entropy) is a more appropriate measure of equivalent choice. It too has certain weaknesses which we discuss. We show that perplexity can also be applied to languages having no obvious statistical description, since an entropy-maximizing probability assignment can be found for any finite-state grammar. Table I shows perplexity values for some well-known speech recognition tasks. 
            Perplexity Vocabulary Dynamic 
            Phone Word size branching factor 
            IBM-Lasers 2.14 21.11 1000 1000 
            IBM-Raleigh 1.69 7.74 250 7.32 
            CMU-AIX05 1.52 6.41 1011 35},
	language = {en},
	number = {S1},
	urldate = {2024-09-03},
	journal = {The Journal of the Acoustical Society of America},
	author = {Jelinek, F. and Mercer, R. L. and Bahl, L. R. and Baker, J. K.},
	month = dec,
	year = {1977},
	pages = {S63--S63},
}

@misc{sai_survey_2020,
	title = {A {Survey} of {Evaluation} {Metrics} {Used} for {NLG} {Systems}},
	url = {http://arxiv.org/abs/2008.12009},
	doi = {10.48550/arXiv.2008.12009},
	abstract = {The success of Deep Learning has created a surge in interest in a wide a range of Natural Language Generation (NLG) tasks. Deep Learning has not only pushed the state of the art in several existing NLG tasks but has also facilitated researchers to explore various newer NLG tasks such as image captioning. Such rapid progress in NLG has necessitated the development of accurate automatic evaluation metrics that would allow us to track the progress in the field of NLG. However, unlike classification tasks, automatically evaluating NLG systems in itself is a huge challenge. Several works have shown that early heuristic-based metrics such as BLEU, ROUGE are inadequate for capturing the nuances in the different NLG tasks. The expanding number of NLG models and the shortcomings of the current metrics has led to a rapid surge in the number of evaluation metrics proposed since 2014. Moreover, various evaluation metrics have shifted from using pre-determined heuristic-based formulae to trained transformer models. This rapid change in a relatively short time has led to the need for a survey of the existing NLG metrics to help existing and new researchers to quickly come up to speed with the developments that have happened in NLG evaluation in the last few years. Through this survey, we first wish to highlight the challenges and difficulties in automatically evaluating NLG systems. Then, we provide a coherent taxonomy of the evaluation metrics to organize the existing metrics and to better understand the developments in the field. We also describe the different metrics in detail and highlight their key contributions. Later, we discuss the main shortcomings identified in the existing metrics and describe the methodology used to evaluate evaluation metrics. Finally, we discuss our suggestions and recommendations on the next steps forward to improve the automatic evaluation metrics.},
	urldate = {2024-08-21},
	publisher = {arXiv},
	author = {Sai, Ananya B. and Mohankumar, Akash Kumar and Khapra, Mitesh M.},
	month = oct,
	year = {2020},
	note = {arXiv:2008.12009 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{calabrese_explainable_2022,
	title = {Explainable {Abuse} {Detection} as {Intent} {Classification} and {Slot} {Filling}},
	volume = {10},
	url = {https://aclanthology.org/2022.tacl-1.82},
	doi = {10.1162/tacl_a_00527},
	abstract = {To proactively offer social media users a safe online experience, there is a need for systems that can detect harmful posts and promptly alert platform moderators. In order to guarantee the enforcement of a consistent policy, moderators are provided with detailed guidelines. In contrast, most state-of-the-art models learn what abuse is from labeled examples and as a result base their predictions on spurious cues, such as the presence of group identifiers, which can be unreliable. In this work we introduce the concept of policy-aware abuse detection, abandoning the unrealistic expectation that systems can reliably learn which phenomena constitute abuse from inspecting the data alone. We propose a machine-friendly representation of the policy that moderators wish to enforce, by breaking it down into a collection of intents and slots. We collect and annotate a dataset of 3,535 English posts with such slots, and show how architectures for intent classification and slot filling can be used for abuse detection, while providing a rationale for model decisions.1},
	urldate = {2024-08-21},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Calabrese, Agostina and Ross, Björn and Lapata, Mirella},
	editor = {Roark, Brian and Nenkova, Ani},
	year = {2022},
	pages = {1440--1454},
}

@inproceedings{sap_social_2020,
	address = {Online},
	title = {Social {Bias} {Frames}: {Reasoning} about {Social} and {Power} {Implications} of {Language}},
	shorttitle = {Social {Bias} {Frames}},
	url = {https://aclanthology.org/2020.acl-main.486},
	doi = {10.18653/v1/2020.acl-main.486},
	abstract = {Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people's judgments about others. For example, given a statement that “we shouldn't lower our standards to hire more women,” most listeners will infer the implicature intended by the speaker - that “women (candidates) are less qualified.” Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80\% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.},
	urldate = {2024-08-09},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sap, Maarten and Gabriel, Saadia and Qin, Lianhui and Jurafsky, Dan and Smith, Noah A. and Choi, Yejin},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {5477--5490},
}

@inproceedings{vidgen_learning_2021,
	address = {Online},
	title = {Learning from the {Worst}: {Dynamically} {Generated} {Datasets} to {Improve} {Online} {Hate} {Detection}},
	shorttitle = {Learning from the {Worst}},
	url = {https://aclanthology.org/2021.acl-long.132},
	doi = {10.18653/v1/2021.acl-long.132},
	abstract = {We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models. We provide a new dataset of 40,000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation. It includes 15,000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate. Hateful entries make up 54\% of the dataset, which is substantially higher than comparable datasets. We show that model performance is substantially improved using this approach. Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. They also have better performance on HateCheck, a suite of functional tests for online hate detection. We provide the code, dataset and annotation guidelines for other researchers to use.},
	urldate = {2024-08-09},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Vidgen, Bertie and Thrush, Tristan and Waseem, Zeerak and Kiela, Douwe},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {1667--1682},
}

@inproceedings{elsherief_latent_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Latent {Hatred}: {A} {Benchmark} for {Understanding} {Implicit} {Hate} {Speech}},
	shorttitle = {Latent {Hatred}},
	url = {https://aclanthology.org/2021.emnlp-main.29},
	doi = {10.18653/v1/2021.emnlp-main.29},
	abstract = {Hate speech has grown significantly on social media, causing serious consequences for victims of all demographics. Despite much attention being paid to characterize and detect discriminatory speech, most work has focused on explicit or overt hate speech, failing to address a more pervasive form based on coded or indirect language. To fill this gap, this work introduces a theoretically-justified taxonomy of implicit hate speech and a benchmark corpus with fine-grained labels for each message and its implication. We present systematic analyses of our dataset using contemporary baselines to detect and explain implicit hate speech, and we discuss key features that challenge existing models. This dataset will continue to serve as a useful benchmark for understanding this multifaceted issue.},
	urldate = {2024-08-09},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {ElSherief, Mai and Ziems, Caleb and Muchlinski, David and Anupindi, Vaishnavi and Seybolt, Jordyn and De Choudhury, Munmun and Yang, Diyi},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {345--363},
}

@inproceedings{alshahrani_optimismpessimism_2021,
	title = {Optimism/{Pessimism} {Prediction} of {Twitter} {Messages} and {Users} {Using} {BERT} with {Soft} {Label} {Assignment}},
	url = {https://ieeexplore.ieee.org/document/9534100},
	doi = {10.1109/IJCNN52387.2021.9534100},
	abstract = {Being able to accurately predict users' outlooks on social media platforms is important for developing educational public health interventions. In this paper, utilizing the contextualized representations provided by BERT, we propose new models to predict optimism/pessimism by fine-tuning BERT. By paying attention to the negation and other syntactic patterns, the self-attention mechanism via the transformer in BERT leads to more accurate models. For example, using the commonly used dataset for optimism/pessimism prediction with the proposed Soft Label Assignment (SLA), we have achieved 100\% prediction accuracy at the user level and 97.10\% at the tweet message level on the test set excluding the neutral ones. Furthermore, utilizing available training sample scores, we assign labels softly, which improves the generalization of the models and therefore their performance further. We additionally analyze the attention heads to illustrate the mechanisms of our models to classify different messages and demonstrate the connections between emotions and optimism/pessimism prediction.},
	urldate = {2024-08-07},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Alshahrani, Ali and Ghaffari, Meysam and Amirizirtol, Kobra and Liu, Xiuwen},
	month = jul,
	year = {2021},
	note = {ISSN: 2161-4407},
	keywords = {BERT, Bit error rate, Blogs, Neural networks, Predictive models, Social networking (online), Syntactics, Training, fine-tuning, soft label, transformer},
	pages = {1--8},
}

@inproceedings{chiang_chatbot_2024,
	title = {Chatbot {Arena}: {An} {Open} {Platform} for {Evaluating} {LLMs} by {Human} {Preference}},
	shorttitle = {Chatbot {Arena}},
	url = {https://openreview.net/forum?id=3MW8GKNyzI},
	abstract = {Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowd-sourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. The platform is publicly available at https://chat.lmsys.org.},
	language = {en},
	urldate = {2024-08-06},
	author = {Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhu, Banghua and Zhang, Hao and Jordan, Michael and Gonzalez, Joseph E. and Stoica, Ion},
	month = jun,
	year = {2024},
}

@inproceedings{arango_monnar_resources_2022,
	address = {Seattle, Washington (Hybrid)},
	title = {Resources for {Multilingual} {Hate} {Speech} {Detection}},
	url = {https://aclanthology.org/2022.woah-1.12},
	doi = {10.18653/v1/2022.woah-1.12},
	abstract = {Most of the published approaches and resources for hate speech detection are tailored for the English language. In consequence, cross-lingual and cross-cultural perspectives lack some essential resources. The lack of diversity of the datasets in Spanish is notable. Variations throughout Spanish-speaking countries make existing datasets not enough to encompass the task in the different Spanish variants. We annotated 9834 tweets from Chile to enrich the existing Spanish resources with different words and new targets of hate that have not been considered in previous studies. We conducted several cross-dataset evaluation experiments of the models published in the literature using our Chilean dataset and two others in English and Spanish. We propose a comparative framework for quickly conducting comparative experiments using different previously published models. In addition, we set up a Codalab competition for further comparison of new models in a standard scenario, that is, data partitions and evaluation metrics. All resources can be accessed trough a centralized repository for researchers to get a complete picture of the progress on the multilingual hate speech and offensive language detection task.},
	urldate = {2024-06-27},
	booktitle = {Proceedings of the {Sixth} {Workshop} on {Online} {Abuse} and {Harms} ({WOAH})},
	publisher = {Association for Computational Linguistics},
	author = {Arango Monnar, Ayme and Perez, Jorge and Poblete, Barbara and Saldaña, Magdalena and Proust, Valentina},
	editor = {Narang, Kanika and Mostafazadeh Davani, Aida and Mathias, Lambert and Vidgen, Bertie and Talat, Zeerak},
	month = jul,
	year = {2022},
	pages = {122--130},
}

@inproceedings{juraska_metricx-23_2023,
	address = {Singapore},
	title = {{MetricX}-23: {The} {Google} {Submission} to the {WMT} 2023 {Metrics} {Shared} {Task}},
	shorttitle = {{MetricX}-23},
	url = {https://aclanthology.org/2023.wmt-1.63},
	doi = {10.18653/v1/2023.wmt-1.63},
	abstract = {This report details the MetricX-23 submission to the WMT23 Metrics Shared Task and provides an overview of the experiments that informed which metrics were submitted. Our 3 submissions—each with a quality estimation (or reference-free) version—are all learned regression-based metrics that vary in the data used for training and which pretrained language model was used for initialization. We report results related to understanding (1) which supervised training data to use, (2) the impact of how the training labels are normalized, (3) the amount of synthetic training data to use, (4) how metric performance is related to model size, and (5) the effect of initializing the metrics with different pretrained language models. The most successful training recipe for MetricX employs two-stage fine-tuning on DA and MQM ratings, and includes synthetic training data. Finally, one important takeaway from our extensive experiments is that optimizing for both segment- and system-level performance at the same time is a challenging task.},
	urldate = {2024-06-26},
	booktitle = {Proceedings of the {Eighth} {Conference} on {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Juraska, Juraj and Finkelstein, Mara and Deutsch, Daniel and Siddhant, Aditya and Mirzazadeh, Mehdi and Freitag, Markus},
	editor = {Koehn, Philipp and Haddow, Barry and Kocmi, Tom and Monz, Christof},
	month = dec,
	year = {2023},
	pages = {756--767},
}

@inproceedings{kocmi_gemba-mqm_2023,
	address = {Singapore},
	title = {{GEMBA}-{MQM}: {Detecting} {Translation} {Quality} {Error} {Spans} with {GPT}-4},
	shorttitle = {{GEMBA}-{MQM}},
	url = {https://aclanthology.org/2023.wmt-1.64},
	doi = {10.18653/v1/2023.wmt-1.64},
	abstract = {This paper introduces GEMBA-MQM, a GPT-based evaluation metric designed to detect translation quality errors, specifically for the quality estimation setting without the need for human reference translations. Based on the power of large language models (LLM), GEMBA-MQM employs a fixed three-shot prompting technique, querying the GPT-4 model to mark error quality spans. Compared to previous works, our method has language-agnostic prompts, thus avoiding the need for manual prompt preparation for new languages. While preliminary results indicate that GEMBA-MQM achieves state-of-the-art accuracy for system ranking, we advise caution when using it in academic works to demonstrate improvements over other methods due to its dependence on the proprietary, black-box GPT model.},
	urldate = {2024-06-26},
	booktitle = {Proceedings of the {Eighth} {Conference} on {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Kocmi, Tom and Federmann, Christian},
	editor = {Koehn, Philipp and Haddow, Barry and Kocmi, Tom and Monz, Christof},
	month = dec,
	year = {2023},
	pages = {768--775},
}

@inproceedings{karpinska_demetr_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{DEMETR}: {Diagnosing} {Evaluation} {Metrics} for {Translation}},
	shorttitle = {{DEMETR}},
	url = {https://aclanthology.org/2022.emnlp-main.649},
	doi = {10.18653/v1/2022.emnlp-main.649},
	abstract = {While machine translation evaluation metrics based on string overlap (e.g., BLEU) have their limitations, their computations are transparent: the BLEU score assigned to a particular candidate translation can be traced back to the presence or absence of certain words. The operations of newer learned metrics (e.g., BLEURT, COMET), which leverage pretrained language models to achieve higher correlations with human quality judgments than BLEU, are opaque in comparison. In this paper, we shed light on the behavior of these learned metrics by creating DEMETR, a diagnostic dataset with 31K English examples (translated from 10 source languages) for evaluating the sensitivity of MT evaluation metrics to 35 different linguistic perturbations spanning semantic, syntactic, and morphological error categories. All perturbations were carefully designed to form minimal pairs with the actual translation (i.e., differ in only one aspect). We find that learned metrics perform substantially better than string-based metrics on DEMETR. Additionally, learned metrics differ in their sensitivity to various phenomena (e.g., BERTScore is sensitive to untranslated words but relatively insensitive to gender manipulation, while COMET is much more sensitive to word repetition than to aspectual changes). We publicly release DEMETR to spur more informed future development of machine translation evaluation metrics},
	urldate = {2024-06-26},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Karpinska, Marzena and Raj, Nishant and Thai, Katherine and Song, Yixiao and Gupta, Ankita and Iyyer, Mohit},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {9540--9561},
}

@misc{guerreiro_xcomet_2023,
	title = {{xCOMET}: {Transparent} {Machine} {Translation} {Evaluation} through {Fine}-grained {Error} {Detection}},
	shorttitle = {{xCOMET}},
	url = {http://arxiv.org/abs/2310.10482},
	doi = {10.48550/arXiv.2310.10482},
	abstract = {Widely used learned metrics for machine translation evaluation, such as COMET and BLEURT, estimate the quality of a translation hypothesis by providing a single sentence-level score. As such, they offer little insight into translation errors (e.g., what are the errors and what is their severity). On the other hand, generative large language models (LLMs) are amplifying the adoption of more granular strategies to evaluation, attempting to detail and categorize translation errors. In this work, we introduce xCOMET, an open-source learned metric designed to bridge the gap between these approaches. xCOMET integrates both sentence-level evaluation and error span detection capabilities, exhibiting state-of-the-art performance across all types of evaluation (sentence-level, system-level, and error span detection). Moreover, it does so while highlighting and categorizing error spans, thus enriching the quality assessment. We also provide a robustness analysis with stress tests, and show that xCOMET is largely capable of identifying localized critical errors and hallucinations.},
	urldate = {2024-06-26},
	publisher = {arXiv},
	author = {Guerreiro, Nuno M. and Rei, Ricardo and van Stigt, Daan and Coheur, Luisa and Colombo, Pierre and Martins, André F. T.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.10482 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{lommel_multidimensional_2014,
	title = {Multidimensional {Quality} {Metrics} ({MQM}): {A} {Framework} for {Declaring} and {Describing} {Translation} {Quality} {Metrics}},
	copyright = {Drets d'autor (c) 2014},
	issn = {1578-7559},
	shorttitle = {Multidimensional {Quality} {Metrics} ({MQM})},
	url = {https://revistes.uab.cat/tradumatica/article/view/n12-lommel-uzskoreit-burchardt},
	doi = {10.5565/rev/tradumatica.77},
	abstract = {En els últims anys l'avaluació de la qualitat de la traducció s'ha convertit en un tema rellevant i a la vegada que, de vegades, polèmic. La perspectiva de la indústria sobre la qualitat està altament fragmentada, en part perquè diferents tipus de projectes de traducció requereixen mètodes molt diferents d'avaluació. A més, els mètodes d'avaluació de la qualitat de les traduccions humanes i de les traduccions elaborades amb traducció automàtica (TA) són d'índole diferent. La manca de claredat provoca incertesa sobre si una traducció compleix amb les necessitats del seu promotor o l'usuari final, i deixa als proveïdors amb dubtes sobre el que els clients volen i necessiten. Com a resposta a aquest fet, el projecte QTLaunchPad, finançat per la Unió Europea, ha desenvolupat el marc denominat Multidimensional Quality Metrics (MQM), un sistema obert i ampliable per declarar i descriure les mètriques sobre qualitat en traducció utilitzant un vocabulari compartit de "classes de problemes”.},
	language = {en},
	number = {12},
	urldate = {2024-06-26},
	journal = {Tradumàtica tecnologies de la traducció},
	author = {Lommel, Arle and Uszkoreit, Hans and Burchardt, Aljoscha},
	month = dec,
	year = {2014},
	keywords = {classes de problemes},
	pages = {455--463},
}

@inproceedings{caselli_hatebert_2021,
	address = {Online},
	title = {{HateBERT}: {Retraining} {BERT} for {Abusive} {Language} {Detection} in {English}},
	shorttitle = {{HateBERT}},
	url = {https://aclanthology.org/2021.woah-1.3},
	doi = {10.18653/v1/2021.woah-1.3},
	abstract = {We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.},
	urldate = {2024-06-26},
	booktitle = {Proceedings of the 5th {Workshop} on {Online} {Abuse} and {Harms} ({WOAH} 2021)},
	publisher = {Association for Computational Linguistics},
	author = {Caselli, Tommaso and Basile, Valerio and Mitrović, Jelena and Granitzer, Michael},
	editor = {Mostafazadeh Davani, Aida and Kiela, Douwe and Lambert, Mathias and Vidgen, Bertie and Prabhakaran, Vinodkumar and Waseem, Zeerak},
	month = aug,
	year = {2021},
	pages = {17--25},
}

@inproceedings{vidgen_learning_2021-1,
	address = {Online},
	title = {Learning from the {Worst}: {Dynamically} {Generated} {Datasets} to {Improve} {Online} {Hate} {Detection}},
	shorttitle = {Learning from the {Worst}},
	url = {https://aclanthology.org/2021.acl-long.132},
	doi = {10.18653/v1/2021.acl-long.132},
	abstract = {We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models. We provide a new dataset of 40,000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation. It includes 15,000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate. Hateful entries make up 54\% of the dataset, which is substantially higher than comparable datasets. We show that model performance is substantially improved using this approach. Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. They also have better performance on HateCheck, a suite of functional tests for online hate detection. We provide the code, dataset and annotation guidelines for other researchers to use.},
	urldate = {2024-06-26},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Vidgen, Bertie and Thrush, Tristan and Waseem, Zeerak and Kiela, Douwe},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {1667--1682},
}

@inproceedings{kim_generalizable_2022,
	address = {Gyeongju, Republic of Korea},
	title = {Generalizable {Implicit} {Hate} {Speech} {Detection} {Using} {Contrastive} {Learning}},
	url = {https://aclanthology.org/2022.coling-1.579},
	abstract = {Hate speech detection has gained increasing attention with the growing prevalence of hateful contents. When a text contains an obvious hate word or expression, it is fairly easy to detect it. However, it is challenging to identify implicit hate speech in nuance or context when there are insufficient lexical cues. Recently, there are several attempts to detect implicit hate speech leveraging pre-trained language models such as BERT and HateBERT. Fine-tuning on an implicit hate speech dataset shows satisfactory performance when evaluated on the test set of the dataset used for training. However, we empirically confirm that the performance drops at least 12.5\%p in F1 score when tested on the dataset that is different from the one used for training. We tackle this cross-dataset underperforming problem using contrastive learning. Based on our observation of common underlying implications in various forms of hate posts, we propose a novel contrastive learning method, ImpCon, that pulls an implication and its corresponding posts close in representation space. We evaluate the effectiveness of ImpCon by running cross-dataset evaluation on three implicit hate speech benchmarks. The experimental results on cross-dataset show that ImpCon improves at most 9.10\% on BERT, and 8.71\% on HateBERT.},
	urldate = {2024-06-26},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Kim, Youngwook and Park, Shinwoo and Han, Yo-Sub},
	editor = {Calzolari, Nicoletta and Huang, Chu-Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key-Sun and Ryu, Pum-Mo and Chen, Hsin-Hsi and Donatelli, Lucia and Ji, Heng and Kurohashi, Sadao and Paggio, Patrizia and Xue, Nianwen and Kim, Seokhwan and Hahm, Younggyun and He, Zhong and Lee, Tony Kyungil and Santus, Enrico and Bond, Francis and Na, Seung-Hoon},
	month = oct,
	year = {2022},
	pages = {6667--6679},
}

@inproceedings{elsherief_latent_2021-1,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Latent {Hatred}: {A} {Benchmark} for {Understanding} {Implicit} {Hate} {Speech}},
	shorttitle = {Latent {Hatred}},
	url = {https://aclanthology.org/2021.emnlp-main.29},
	doi = {10.18653/v1/2021.emnlp-main.29},
	abstract = {Hate speech has grown significantly on social media, causing serious consequences for victims of all demographics. Despite much attention being paid to characterize and detect discriminatory speech, most work has focused on explicit or overt hate speech, failing to address a more pervasive form based on coded or indirect language. To fill this gap, this work introduces a theoretically-justified taxonomy of implicit hate speech and a benchmark corpus with fine-grained labels for each message and its implication. We present systematic analyses of our dataset using contemporary baselines to detect and explain implicit hate speech, and we discuss key features that challenge existing models. This dataset will continue to serve as a useful benchmark for understanding this multifaceted issue.},
	urldate = {2024-06-26},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {ElSherief, Mai and Ziems, Caleb and Muchlinski, David and Anupindi, Vaishnavi and Seybolt, Jordyn and De Choudhury, Munmun and Yang, Diyi},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {345--363},
}

@misc{kanumolu_unsupervised_2023,
	title = {Unsupervised {Approach} to {Evaluate} {Sentence}-{Level} {Fluency}: {Do} {We} {Really} {Need} {Reference}?},
	shorttitle = {Unsupervised {Approach} to {Evaluate} {Sentence}-{Level} {Fluency}},
	url = {http://arxiv.org/abs/2312.01500},
	doi = {10.48550/arXiv.2312.01500},
	abstract = {Fluency is a crucial goal of all Natural Language Generation (NLG) systems. Widely used automatic evaluation metrics fall short in capturing the fluency of machine-generated text. Assessing the fluency of NLG systems poses a challenge since these models are not limited to simply reusing words from the input but may also generate abstractions. Existing reference-based fluency evaluations, such as word overlap measures, often exhibit weak correlations with human judgments. This paper adapts an existing unsupervised technique for measuring text fluency without the need for any reference. Our approach leverages various word embeddings and trains language models using Recurrent Neural Network (RNN) architectures. We also experiment with other available multilingual Language Models (LMs). To assess the performance of the models, we conduct a comparative analysis across 10 Indic languages, correlating the obtained fluency scores with human judgments. Our code and human-annotated benchmark test-set for fluency is available at https://github.com/AnanyaCoder/TextFluencyForIndicLanaguges.},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Kanumolu, Gopichand and Madasu, Lokesh and Baswani, Pavan and Mukherjee, Ananya and Shrivastava, Manish},
	month = dec,
	year = {2023},
	note = {arXiv:2312.01500 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{sai_perturbation_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Perturbation {CheckLists} for {Evaluating} {NLG} {Evaluation} {Metrics}},
	url = {https://aclanthology.org/2021.emnlp-main.575},
	doi = {10.18653/v1/2021.emnlp-main.575},
	abstract = {Natural Language Generation (NLG) evaluation is a multifaceted task requiring assessment of multiple desirable criteria, e.g., fluency, coherency, coverage, relevance, adequacy, overall quality, etc. Across existing datasets for 6 NLG tasks, we observe that the human evaluation scores on these multiple criteria are often not correlated. For example, there is a very low correlation between human scores on fluency and data coverage for the task of structured data to text generation. This suggests that the current recipe of proposing new automatic evaluation metrics for NLG by showing that they correlate well with scores assigned by humans for a single criteria (overall quality) alone is inadequate. Indeed, our extensive study involving 25 automatic evaluation metrics across 6 different tasks and 18 different evaluation criteria shows that there is no single metric which correlates well with human scores on all desirable criteria, for most NLG tasks. Given this situation, we propose CheckLists for better design and evaluation of automatic metrics. We design templates which target a specific criteria (e.g., coverage) and perturb the output such that the quality gets affected only along this specific criteria (e.g., the coverage drops). We show that existing evaluation metrics are not robust against even such simple perturbations and disagree with scores assigned by humans to the perturbed output. The proposed templates thus allow for a fine-grained assessment of automatic evaluation metrics exposing their limitations and will facilitate better design, analysis and evaluation of such metrics. Our templates and code are available at https://iitmnlp.github.io/EvalEval/},
	urldate = {2024-06-05},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Sai, Ananya B. and Dixit, Tanay and Sheth, Dev Yashpal and Mohan, Sreyas and Khapra, Mitesh M.},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {7219--7234},
}

@inproceedings{nimah_nlg_2023,
	address = {Toronto, Canada},
	title = {{NLG} {Evaluation} {Metrics} {Beyond} {Correlation} {Analysis}: {An} {Empirical} {Metric} {Preference} {Checklist}},
	shorttitle = {{NLG} {Evaluation} {Metrics} {Beyond} {Correlation} {Analysis}},
	url = {https://aclanthology.org/2023.acl-long.69},
	doi = {10.18653/v1/2023.acl-long.69},
	abstract = {In this study, we analyze automatic evaluation metrics for Natural Language Generation (NLG), specifically task-agnostic metrics and human-aligned metrics. Task-agnostic metrics, such as Perplexity, BLEU, BERTScore, are cost-effective and highly adaptable to diverse NLG tasks, yet they have a weak correlation with human. Human-aligned metrics (CTC, CtrlEval, UniEval) improves correlation level by incorporating desirable human-like qualities as training objective. However, their effectiveness at discerning system-level performance and quality of system outputs remain unclear. We present metric preference checklist as a framework to assess the effectiveness of automatic metrics in three NLG tasks: Text Summarization, Dialogue Response Generation, and Controlled Generation. Our proposed framework provides access: (i) for verifying whether automatic metrics are faithful to human preference, regardless of their correlation level to human; and (ii) for inspecting the strengths and limitations of NLG systems via pairwise evaluation. We show that automatic metrics provide a better guidance than human on discriminating system-level performance in Text Summarization and Controlled Generation tasks. We also show that multi-aspect human-aligned metric (UniEval) is not necessarily dominant over single-aspect human-aligned metrics (CTC, CtrlEval) and task-agnostic metrics (BLEU, BERTScore), particularly in Controlled Generation tasks.},
	urldate = {2024-06-05},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Nimah, Iftitahu and Fang, Meng and Menkovski, Vlado and Pechenizkiy, Mykola},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {1240--1266},
}

@article{ruan_better_2024,
	title = {Better than {Random}: {Reliable} {NLG} {Human} {Evaluation} with {Constrained} {Active} {Sampling}},
	volume = {38},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Better than {Random}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29857},
	doi = {10.1609/aaai.v38i17.29857},
	abstract = {Human evaluation is viewed as a reliable evaluation method for NLG which is expensive and time-consuming. To save labor and costs, researchers usually perform human evaluation on a small subset of data sampled from the whole dataset in practice. However, different selection subsets will lead to different rankings of the systems. To give a more correct inter-system ranking and make the gold standard human evaluation more reliable, we propose a Constrained Active Sampling Framework (CASF) for reliable human judgment. CASF operates through a Learner, a Systematic Sampler and a Constrained Controller to select representative samples for getting a more correct inter-system ranking. Experiment results on 137 real NLG evaluation setups with 44 human evaluation metrics across 16 datasets and 5 NLG tasks demonstrate CASF receives 93.18{\textbackslash}\% top-ranked system recognition accuracy and ranks first or ranks second on 90.91{\textbackslash}\% of the human metrics with 0.83 overall inter-system ranking Kendall correlation. Code and data are publicly available online.},
	number = {17},
	urldate = {2024-06-05},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ruan, Jie and Pu, Xiao and Gao, Mingqi and Wan, Xiaojun and Zhu, Yuesheng},
	month = mar,
	year = {2024},
	pages = {18915--18923},
}

@inproceedings{salazar_masked_2020,
	address = {Online},
	title = {Masked {Language} {Model} {Scoring}},
	url = {https://aclanthology.org/2020.acl-main.240},
	doi = {10.18653/v1/2020.acl-main.240},
	abstract = {Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model's WER by 30\% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL's unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.},
	urldate = {2024-06-05},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Salazar, Julian and Liang, Davis and Nguyen, Toan Q. and Kirchhoff, Katrin},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {2699--2712},
}

@inproceedings{chen_storyer_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{StoryER}: {Automatic} {Story} {Evaluation} via {Ranking}, {Rating} and {Reasoning}},
	shorttitle = {{StoryER}},
	url = {https://aclanthology.org/2022.emnlp-main.114},
	doi = {10.18653/v1/2022.emnlp-main.114},
	abstract = {Existing automatic story evaluation methods place a premium on story lexical level coherence, deviating from human preference.We go beyond this limitation by considering a novel Story Evaluation method that mimics human preference when judging a story, namely StoryER, which consists of three sub-tasks: Ranking, Rating and Reasoning.Given either a machine-generated or a human-written story, StoryER requires the machine to output 1) a preference score that corresponds to human preference, 2) specific ratings and their corresponding confidences and 3) comments for various aspects (e.g., opening, character-shaping).To support these tasks, we introduce a well-annotated dataset comprising (i) 100k ranked story pairs; and (ii) a set of 46k ratings and comments on various aspects of the story.We finetune Longformer-Encoder-Decoder (LED) on the collected dataset, with the encoder responsible for preference score and aspect prediction and the decoder for comment generation.Our comprehensive experiments result a competitive benchmark for each task, showing the high correlation to human preference.In addition, we have witnessed the joint learning of the preference scores, the aspect ratings, and the comments brings gain each single task.Our dataset and benchmarks are publicly available to advance the research of story evaluation tasks.},
	urldate = {2024-05-19},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Hong and Vo, Duc and Takamura, Hiroya and Miyao, Yusuke and Nakayama, Hideki},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {1739--1753},
}

@article{fabbri_summeval_2021,
	title = {{SummEval}: {Re}-evaluating {Summarization} {Evaluation}},
	volume = {9},
	shorttitle = {{SummEval}},
	url = {https://aclanthology.org/2021.tacl-1.24},
	doi = {10.1162/tacl_a_00373},
	abstract = {The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations; 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format; 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics; and 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments.},
	urldate = {2024-05-19},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Fabbri, Alexander R. and Kryściński, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
	editor = {Roark, Brian and Nenkova, Ani},
	year = {2021},
	pages = {391--409},
}

@inproceedings{bhandari_re-evaluating_2020,
	address = {Online},
	title = {Re-evaluating {Evaluation} in {Text} {Summarization}},
	url = {https://aclanthology.org/2020.emnlp-main.751},
	doi = {10.18653/v1/2020.emnlp-main.751},
	abstract = {Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not – for nearly 20 years ROUGE has been the standard evaluation in most summarization papers. In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive).},
	urldate = {2024-05-19},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Bhandari, Manik and Gour, Pranav Narayan and Ashfaq, Atabak and Liu, Pengfei and Neubig, Graham},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {9347--9359},
}

@misc{grusky_newsroom_2020,
	title = {Newsroom: {A} {Dataset} of 1.3 {Million} {Summaries} with {Diverse} {Extractive} {Strategies}},
	shorttitle = {Newsroom},
	url = {http://arxiv.org/abs/1804.11283},
	doi = {10.48550/arXiv.1804.11283},
	abstract = {We present NEWSROOM, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications. Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles. In particular, the summaries combine abstractive and extractive strategies, borrowing words and phrases from articles at varying rates. We analyze the extraction strategies used in NEWSROOM summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges.},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Grusky, Max and Naaman, Mor and Artzi, Yoav},
	month = may,
	year = {2020},
	note = {arXiv:1804.11283 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{narayan_dont_2018,
	title = {Don't {Give} {Me} the {Details}, {Just} the {Summary}! {Topic}-{Aware} {Convolutional} {Neural} {Networks} for {Extreme} {Summarization}},
	url = {http://arxiv.org/abs/1808.08745},
	doi = {10.48550/arXiv.1808.08745},
	abstract = {We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question "What is the article about?". We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article's topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.},
	urldate = {2024-05-19},
	publisher = {arXiv},
	author = {Narayan, Shashi and Cohen, Shay B. and Lapata, Mirella},
	month = aug,
	year = {2018},
	note = {arXiv:1808.08745 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{fan_hierarchical_2018,
	address = {Melbourne, Australia},
	title = {Hierarchical {Neural} {Story} {Generation}},
	url = {https://aclanthology.org/P18-1082},
	doi = {10.18653/v1/P18-1082},
	abstract = {We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.},
	urldate = {2024-05-15},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Fan, Angela and Lewis, Mike and Dauphin, Yann},
	editor = {Gurevych, Iryna and Miyao, Yusuke},
	month = jul,
	year = {2018},
	pages = {889--898},
}

@inproceedings{liu_g-eval_2023,
	address = {Singapore},
	title = {G-{Eval}: {NLG} {Evaluation} using {Gpt}-4 with {Better} {Human} {Alignment}},
	shorttitle = {G-{Eval}},
	url = {https://aclanthology.org/2023.emnlp-main.153},
	doi = {10.18653/v1/2023.emnlp-main.153},
	abstract = {The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose analysis on the behavior of LLM-based evaluators, and highlight the potential concern of LLM-based evaluators having a bias towards the LLM-generated texts.},
	urldate = {2024-05-15},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {2511--2522},
}

@misc{liu_divergence_2021,
	title = {Divergence {Frontiers} for {Generative} {Models}: {Sample} {Complexity}, {Quantization} {Effects}, and {Frontier} {Integrals}},
	shorttitle = {Divergence {Frontiers} for {Generative} {Models}},
	url = {http://arxiv.org/abs/2106.07898},
	doi = {10.48550/arXiv.2106.07898},
	abstract = {The spectacular success of deep generative models calls for quantitative tools to measure their statistical performance. Divergence frontiers have recently been proposed as an evaluation framework for generative models, due to their ability to measure the quality-diversity trade-off inherent to deep generative modeling. We establish non-asymptotic bounds on the sample complexity of divergence frontiers. We also introduce frontier integrals which provide summary statistics of divergence frontiers. We show how smoothed estimators such as Good-Turing or Krichevsky-Trofimov can overcome the missing mass problem and lead to faster rates of convergence. We illustrate the theoretical results with numerical examples from natural language processing and computer vision.},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Liu, Lang and Pillutla, Krishna and Welleck, Sean and Oh, Sewoong and Choi, Yejin and Harchaoui, Zaid},
	month = dec,
	year = {2021},
	note = {arXiv:2106.07898 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{glover_revisiting_2022,
	address = {Abu Dhabi, United Arab Emirates (Hybrid)},
	title = {Revisiting text decomposition methods for {NLI}-based factuality scoring of summaries},
	url = {https://aclanthology.org/2022.gem-1.7},
	doi = {10.18653/v1/2022.gem-1.7},
	abstract = {Scoring the factuality of a generated summary involves measuring the degree to which a target text contains factual information using the input document as support. Given the similarities in the problem formulation, previous work has shown that Natural Language Inference models can be effectively repurposed to perform this task. As these models are trained to score entailment at a sentence level, several recent studies have shown that decomposing either the input document or the summary into sentences helps with factuality scoring. But is fine-grained decomposition always a winning strategy? In this paper we systematically compare different granularities of decomposition - from document to sub-sentence level, and we show that the answer is no. Our results show that incorporating additional context can yield improvement, but that this does not necessarily apply to all datasets. We also show that small changes to previously proposed entailment-based scoring methods can result in better performance, highlighting the need for caution in model and methodology selection for downstream tasks.},
	urldate = {2024-05-13},
	booktitle = {Proceedings of the 2nd {Workshop} on {Natural} {Language} {Generation}, {Evaluation}, and {Metrics} ({GEM})},
	publisher = {Association for Computational Linguistics},
	author = {Glover, John and Fancellu, Federico and Jagannathan, Vasudevan and Gormley, Matthew R. and Schaaf, Thomas},
	editor = {Bosselut, Antoine and Chandu, Khyathi and Dhole, Kaustubh and Gangal, Varun and Gehrmann, Sebastian and Jernite, Yacine and Novikova, Jekaterina and Perez-Beltrachini, Laura},
	month = dec,
	year = {2022},
	pages = {97--105},
}

@misc{sellam_bleurt_2020,
	title = {{BLEURT}: {Learning} {Robust} {Metrics} for {Text} {Generation}},
	shorttitle = {{BLEURT}},
	url = {https://arxiv.org/abs/2004.04696v5},
	abstract = {Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgments. We propose BLEURT, a learned evaluation metric based on BERT that can model human judgments with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG Competition dataset. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.},
	language = {en},
	urldate = {2024-05-13},
	journal = {arXiv.org},
	author = {Sellam, Thibault and Das, Dipanjan and Parikh, Ankur P.},
	month = apr,
	year = {2020},
}

@misc{kane_nubia_2020,
	title = {{NUBIA}: {NeUral} {Based} {Interchangeability} {Assessor} for {Text} {Generation}},
	shorttitle = {{NUBIA}},
	url = {https://arxiv.org/abs/2004.14667v2},
	abstract = {We present NUBIA, a methodology to build automatic evaluation metrics for text generation using only machine learning models as core components. A typical NUBIA model is composed of three modules: a neural feature extractor, an aggregator and a calibrator. We demonstrate an implementation of NUBIA which outperforms metrics currently used to evaluate machine translation, summaries and slightly exceeds/matches state of the art metrics on correlation with human judgement on the WMT segment-level Direct Assessment task, sentence-level ranking and image captioning evaluation. The model implemented is modular, explainable and set to continuously improve over time.},
	language = {en},
	urldate = {2024-05-13},
	journal = {arXiv.org},
	author = {Kane, Hassan and Kocyigit, Muhammed Yusuf and Abdalla, Ali and Ajanoh, Pelkins and Coulibali, Mohamed},
	month = apr,
	year = {2020},
}

@misc{brake_comparing_2024,
	title = {Comparing {Two} {Model} {Designs} for {Clinical} {Note} {Generation}; {Is} an {LLM} a {Useful} {Evaluator} of {Consistency}?},
	url = {https://arxiv.org/abs/2404.06503v1},
	abstract = {Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1\% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.},
	language = {en},
	urldate = {2024-05-13},
	journal = {arXiv.org},
	author = {Brake, Nathan and Schaaf, Thomas},
	month = apr,
	year = {2024},
}

@inproceedings{yuan_bartscore_2021,
	title = {{BARTScore}: {Evaluating} {Generated} {Text} as {Text} {Generation}},
	volume = {34},
	shorttitle = {{BARTScore}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html},
	abstract = {A wide variety of NLP applications, such as machine translation, summarization, and dialog, involve text generation. One major challenge for these applications is how to evaluate whether such generated texts are actually fluent, accurate, or effective. In this work, we conceptualize the evaluation of generated text as a text generation problem, modeled using pre-trained sequence-to-sequence models. The general idea is that models trained to convert the generated text to/from a reference output or the source text will achieve higher scores when the generated text is better. We operationalize this idea using BART, an encoder-decoder based pre-trained model, and propose a metric BARTScore with a number of variants that can be flexibly applied in an unsupervised fashion to evaluation of text from different perspectives (e.g. informativeness, fluency, or factuality). BARTScore is conceptually simple and empirically effective. It can outperform existing top-scoring metrics in 16 of 22 test settings, covering evaluation of 16 datasets (e.g., machine translation, text summarization) and 7 different perspectives (e.g., informativeness, factuality). Code to calculate BARTScore is available at https://github.com/neulab/BARTScore, and we have released an interactive leaderboard for meta-evaluation at http://explainaboard.nlpedia.ai/leaderboard/task-meval/ on the ExplainaBoard platform, which allows us to interactively understand the strengths, weaknesses, and complementarity of each metric.},
	urldate = {2024-05-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
	year = {2021},
	pages = {27263--27277},
}

@misc{fu_gptscore_2023,
	title = {{GPTScore}: {Evaluate} as {You} {Desire}},
	shorttitle = {{GPTScore}},
	url = {https://arxiv.org/abs/2302.04166v2},
	abstract = {Generative Artificial Intelligence (AI) has enabled the development of sophisticated models that are capable of producing high-caliber text, images, and other outputs through the utilization of large pre-trained models. Nevertheless, assessing the quality of the generation is an even more arduous task than the generation itself, and this issue has not been given adequate consideration recently. This paper proposes a novel evaluation framework, GPTScore, which utilizes the emergent abilities (e.g., zero-shot instruction) of generative pre-trained models to score generated texts. There are 19 pre-trained models explored in this paper, ranging in size from 80M (e.g., FLAN-T5-small) to 175B (e.g., GPT3). Experimental results on four text generation tasks, 22 evaluation aspects, and corresponding 37 datasets demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions. This nature helps us overcome several long-standing challenges in text evaluation--how to achieve customized, multi-faceted evaluation without the need for annotated samples. We make our code publicly available at https://github.com/jinlanfu/GPTScore.},
	language = {en},
	urldate = {2024-05-13},
	journal = {arXiv.org},
	author = {Fu, Jinlan and Ng, See-Kiong and Jiang, Zhengbao and Liu, Pengfei},
	month = feb,
	year = {2023},
}

@article{pillutla_mauve_2023,
	title = {{MAUVE} {Scores} for {Generative} {Models}: {Theory} and {Practice}},
	volume = {24},
	issn = {1533-7928},
	shorttitle = {{MAUVE} {Scores} for {Generative} {Models}},
	url = {http://jmlr.org/papers/v24/23-0023.html},
	abstract = {Generative artificial intelligence has made significant strides, producing text indistinguishable from human prose and remarkably photorealistic images. Automatically measuring how close the generated data distribution is to the target distribution is central to diagnosing existing models and developing better ones. We present MAUVE, a family of comparison measures between pairs of distributions such as those encountered in the generative modeling of text or images. These scores are statistical summaries of divergence frontiers capturing two types of errors in generative modeling. We explore three approaches to statistically estimate these scores: vector quantization, non-parametric estimation, and classifier-based estimation. We provide statistical bounds for the vector quantization approach. Empirically, we find that the proposed scores paired with a range of \$f\$-divergences and statistical estimation methods can quantify the gaps between the distributions of human-written text and those of modern neural language models by correlating with human judgments and identifying known properties of the generated texts. We demonstrate in the vision domain that MAUVE can identify known properties of generated images on par with or better than existing metrics. In conclusion, we present practical recommendations for using MAUVE effectively with language and image modalities.},
	number = {356},
	urldate = {2024-05-12},
	journal = {Journal of Machine Learning Research},
	author = {Pillutla, Krishna and Liu, Lang and Thickstun, John and Welleck, Sean and Swayamdipta, Swabha and Zellers, Rowan and Oh, Sewoong and Choi, Yejin and Harchaoui, Zaid},
	year = {2023},
	pages = {1--92},
}

@inproceedings{turian_evaluation_2003,
	address = {New Orleans, USA},
	title = {Evaluation of machine translation and its evaluation},
	url = {https://aclanthology.org/2003.mtsummit-papers.51},
	abstract = {Evaluation of MT evaluation measures is limited by inconsistent human judgment data. Nonetheless, machine translation can be evaluated using the well-known measures precision, recall, and their average, the F-measure. The unigram-based F-measure has significantly higher correlation with human judgments than recently proposed alternatives. More importantly, this standard measure has an intuitive graphical interpretation, which can facilitate insight into how MT systems might be improved. The relevant software is publicly available from http://nlp.cs.nyu.edu/GTM/.},
	urldate = {2024-05-12},
	booktitle = {Proceedings of {Machine} {Translation} {Summit} {IX}: {Papers}},
	author = {Turian, Joseph P. and Shen, Luke and Melamed, I. Dan},
	month = sep,
	year = {2003},
}

@inproceedings{doddington_automatic_2002,
	address = {San Francisco, CA, USA},
	series = {{HLT} '02},
	title = {Automatic evaluation of machine translation quality using n-gram co-occurrence statistics},
	abstract = {Evaluation is recognized as an extremely helpful forcing function in Human Language Technology R\&D. Unfortunately, evaluation has not been a very powerful tool in machine translation (MT) research because it requires human judgments and is thus expensive and time-consuming and not easily factored into the MT research agenda. However, at the July 2001 TIDES PI meeting in Philadelphia, IBM described an automatic MT evaluation technique that can provide immediate feedback and guidance in MT research. Their idea, which they call an "evaluation understudy", compares MT output with expert reference translations in terms of the statistics of short sequences of words (word N-grams). The more of these N-grams that a translation shares with the reference translations, the better the translation is judged to be. The idea is elegant in its simplicity. But far more important, IBM showed a strong correlation between these automatically generated scores and human judgments of translation quality. As a result, DARPA commissioned NIST to develop an MT evaluation facility based on the IBM work. This utility is now available from NIST and serves as the primary evaluation measure for TIDES MT research.},
	urldate = {2024-05-11},
	booktitle = {Proceedings of the second international conference on {Human} {Language} {Technology} {Research}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Doddington, George},
	month = mar,
	year = {2002},
	pages = {138--145},
}

@inproceedings{papineni_bleu_2002,
	address = {Philadelphia, Pennsylvania, USA},
	title = {Bleu: a {Method} for {Automatic} {Evaluation} of {Machine} {Translation}},
	shorttitle = {Bleu},
	url = {https://aclanthology.org/P02-1040},
	doi = {10.3115/1073083.1073135},
	urldate = {2024-05-11},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	editor = {Isabelle, Pierre and Charniak, Eugene and Lin, Dekang},
	month = jul,
	year = {2002},
	pages = {311--318},
}

@misc{dhingra_handling_2019,
	title = {Handling {Divergent} {Reference} {Texts} when {Evaluating} {Table}-to-{Text} {Generation}},
	url = {http://arxiv.org/abs/1906.01081},
	doi = {10.48550/arXiv.1906.01081},
	abstract = {Automatically constructed datasets for generating text from semi-structured data (tables), such as WikiBio, often contain reference texts that diverge from the information in the corresponding semi-structured data. We show that metrics which rely solely on the reference texts, such as BLEU and ROUGE, show poor correlation with human judgments when those references diverge. We propose a new metric, PARENT, which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall. Through a large scale human evaluation study of table-to-text models for WikiBio, we show that PARENT correlates with human judgments better than existing text generation metrics. We also adapt and evaluate the information extraction based evaluation proposed by Wiseman et al (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Dhingra, Bhuwan and Faruqui, Manaal and Parikh, Ankur and Chang, Ming-Wei and Das, Dipanjan and Cohen, William W.},
	month = jun,
	year = {2019},
	note = {arXiv:1906.01081 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{lowe_towards_2018,
	title = {Towards an {Automatic} {Turing} {Test}: {Learning} to {Evaluate} {Dialogue} {Responses}},
	shorttitle = {Towards an {Automatic} {Turing} {Test}},
	url = {http://arxiv.org/abs/1708.07149},
	doi = {10.48550/arXiv.1708.07149},
	abstract = {Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality. Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We present an evaluation model (ADEM) that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model's predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue models unseen during training, an important step for automatic dialogue evaluation.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Lowe, Ryan and Noseworthy, Michael and Serban, Iulian V. and Angelard-Gontier, Nicolas and Bengio, Yoshua and Pineau, Joelle},
	month = jan,
	year = {2018},
	note = {arXiv:1708.07149 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{mathur_putting_2019,
	address = {Florence, Italy},
	title = {Putting {Evaluation} in {Context}: {Contextual} {Embeddings} {Improve} {Machine} {Translation} {Evaluation}},
	shorttitle = {Putting {Evaluation} in {Context}},
	url = {https://aclanthology.org/P19-1269},
	doi = {10.18653/v1/P19-1269},
	abstract = {Accurate, automatic evaluation of machine translation is critical for system tuning, and evaluating progress in the field. We proposed a simple unsupervised metric, and additional supervised metrics which rely on contextual word embeddings to encode the translation and reference sentences. We find that these models rival or surpass all existing metrics in the WMT 2017 sentence-level and system-level tracks, and our trained model has a substantially higher correlation with human judgements than all existing metrics on the WMT 2017 to-English sentence level dataset.},
	urldate = {2024-05-08},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Mathur, Nitika and Baldwin, Timothy and Cohn, Trevor},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {2799--2808},
}

@misc{nema_towards_2018,
	title = {Towards a {Better} {Metric} for {Evaluating} {Question} {Generation} {Systems}},
	url = {http://arxiv.org/abs/1808.10192},
	doi = {10.48550/arXiv.1808.10192},
	abstract = {There has always been criticism for using \$n\$-gram based similarity metrics, such as BLEU, NIST, etc, for evaluating the performance of NLG systems. However, these metrics continue to remain popular and are recently being used for evaluating the performance of systems which automatically generate questions from documents, knowledge graphs, images, etc. Given the rising interest in such automatic question generation (AQG) systems, it is important to objectively examine whether these metrics are suitable for this task. In particular, it is important to verify whether such metrics used for evaluating AQG systems focus on answerability of the generated question by preferring questions which contain all relevant information such as question type (Wh-types), entities, relations, etc. In this work, we show that current automatic evaluation metrics based on \$n\$-gram similarity do not always correlate well with human judgments about answerability of a question. To alleviate this problem and as a first step towards better evaluation metrics for AQG, we introduce a scoring function to capture answerability and show that when this scoring function is integrated with existing metrics, they correlate significantly better with human judgments. The scripts and data developed as a part of this work are made publicly available at https://github.com/PrekshaNema25/Answerability-Metric},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Nema, Preksha and Khapra, Mitesh M.},
	month = aug,
	year = {2018},
	note = {arXiv:1808.10192 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{lin_rouge_2004,
	address = {Barcelona, Spain},
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	shorttitle = {{ROUGE}},
	url = {https://aclanthology.org/W04-1013},
	urldate = {2024-05-08},
	booktitle = {Text {Summarization} {Branches} {Out}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew},
	month = jul,
	year = {2004},
	pages = {74--81},
}

@inproceedings{banerjee_meteor_2005,
	address = {Ann Arbor, Michigan},
	title = {{METEOR}: {An} {Automatic} {Metric} for {MT} {Evaluation} with {Improved} {Correlation} with {Human} {Judgments}},
	shorttitle = {{METEOR}},
	url = {https://aclanthology.org/W05-0909},
	urldate = {2024-05-08},
	booktitle = {Proceedings of the {ACL} {Workshop} on {Intrinsic} and {Extrinsic} {Evaluation} {Measures} for {Machine} {Translation} and/or {Summarization}},
	publisher = {Association for Computational Linguistics},
	author = {Banerjee, Satanjeev and Lavie, Alon},
	editor = {Goldstein, Jade and Lavie, Alon and Lin, Chin-Yew and Voss, Clare},
	month = jun,
	year = {2005},
	pages = {65--72},
}

@inproceedings{papineni_bleu_2002-1,
	address = {Philadelphia, Pennsylvania, USA},
	title = {Bleu: a {Method} for {Automatic} {Evaluation} of {Machine} {Translation}},
	shorttitle = {Bleu},
	url = {https://aclanthology.org/P02-1040},
	doi = {10.3115/1073083.1073135},
	urldate = {2024-05-08},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	editor = {Isabelle, Pierre and Charniak, Eugene and Lin, Dekang},
	month = jul,
	year = {2002},
	pages = {311--318},
}

@article{messina_survey_2022,
	title = {A {Survey} on {Deep} {Learning} and {Explainability} for {Automatic} {Report} {Generation} from {Medical} {Images}},
	volume = {54},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3522747},
	doi = {10.1145/3522747},
	abstract = {Every year physicians face an increasing demand of image-based diagnosis from patients, a problem that can be addressed with recent artificial intelligence methods. In this context, we survey works in the area of automatic report generation from medical images, with emphasis on methods using deep neural networks, with respect to (1) Datasets, (2) Architecture Design, (3) Explainability, and (4) Evaluation Metrics. Our survey identifies interesting developments but also remaining challenges. Among them, the current evaluation of generated reports is especially weak, since it mostly relies on traditional Natural Language Processing (NLP) metrics, which do not accurately capture medical correctness.},
	number = {10s},
	urldate = {2024-05-08},
	journal = {ACM Computing Surveys},
	author = {Messina, Pablo and Pino, Pablo and Parra, Denis and Soto, Alvaro and Besa, Cecilia and Uribe, Sergio and Andía, Marcelo and Tejos, Cristian and Prieto, Claudia and Capurro, Daniel},
	month = sep,
	year = {2022},
	keywords = {Medical report generation, deep learning, explainable artificial intelligence, medical image captioning, medical images, natural language report},
	pages = {203:1--203:40},
}

@article{goyal_survey_2023,
	title = {A {Survey} of {Adversarial} {Defenses} and {Robustness} in {NLP}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3593042},
	doi = {10.1145/3593042},
	abstract = {In the past few years, it has become increasingly evident that deep neural networks are not resilient enough to withstand adversarial perturbations in input data, leaving them vulnerable to attack. Various authors have proposed strong adversarial attacks for computer vision and Natural Language Processing (NLP) tasks. As a response, many defense mechanisms have also been proposed to prevent these networks from failing. The significance of defending neural networks against adversarial attacks lies in ensuring that the model’s predictions remain unchanged even if the input data is perturbed. Several methods for adversarial defense in NLP have been proposed, catering to different NLP tasks such as text classification, named entity recognition, and natural language inference. Some of these methods not only defend neural networks against adversarial attacks but also act as a regularization mechanism during training, saving the model from overfitting. This survey aims to review the various methods proposed for adversarial defenses in NLP over the past few years by introducing a novel taxonomy. The survey also highlights the fragility of advanced deep neural networks in NLP and the challenges involved in defending them.},
	number = {14s},
	urldate = {2024-05-08},
	journal = {ACM Computing Surveys},
	author = {Goyal, Shreya and Doddapaneni, Sumanth and Khapra, Mitesh M. and Ravindran, Balaraman},
	month = jul,
	year = {2023},
	keywords = {Adversarial attacks, NLP, adversarial defenses, perturbations},
	pages = {332:1--332:39},
}

@article{madsen_post-hoc_2022,
	title = {Post-hoc {Interpretability} for {Neural} {NLP}: {A} {Survey}},
	volume = {55},
	issn = {0360-0300},
	shorttitle = {Post-hoc {Interpretability} for {Neural} {NLP}},
	url = {https://dl.acm.org/doi/10.1145/3546577},
	doi = {10.1145/3546577},
	abstract = {Neural networks for NLP are becoming increasingly complex and widespread, and there is a growing concern if these models are responsible to use. Explaining models helps to address the safety and ethical concerns and is essential for accountability. Interpretability serves to provide these explanations in terms that are understandable to humans. Additionally, post-hoc methods provide explanations after a model is learned and are generally model-agnostic. This survey provides a categorization of how recent post-hoc interpretability methods communicate explanations to humans, it discusses each method in-depth, and how they are validated, as the latter is often a common concern.},
	number = {8},
	urldate = {2024-05-08},
	journal = {ACM Computing Surveys},
	author = {Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
	month = dec,
	year = {2022},
	keywords = {Interpretability, post-hoc explanations, transparency},
	pages = {155:1--155:42},
}

@misc{wang_is_2023,
	title = {Is {ChatGPT} a {Good} {NLG} {Evaluator}? {A} {Preliminary} {Study}},
	shorttitle = {Is {ChatGPT} a {Good} {NLG} {Evaluator}?},
	url = {https://arxiv.org/abs/2303.04048v3},
	abstract = {Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric. In this report, we provide a preliminary meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG meta-evaluation datasets (including summarization, story generation and data-to-text tasks). Experimental results show that compared with previous automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation with human judgments in most cases. In addition, we find that the effectiveness of the ChatGPT evaluator might be influenced by the creation method of the meta-evaluation datasets. For the meta-evaluation datasets which are created greatly depending on the reference and thus are biased, the ChatGPT evaluator might lose its effectiveness. We hope our preliminary study could prompt the emergence of a general-purposed reliable NLG metric.},
	language = {en},
	urldate = {2024-05-08},
	journal = {arXiv.org},
	author = {Wang, Jiaan and Liang, Yunlong and Meng, Fandong and Sun, Zengkui and Shi, Haoxiang and Li, Zhixu and Xu, Jinan and Qu, Jianfeng and Zhou, Jie},
	month = mar,
	year = {2023},
}

@misc{sai_perturbation_2021-1,
	title = {Perturbation {CheckLists} for {Evaluating} {NLG} {Evaluation} {Metrics}},
	url = {http://arxiv.org/abs/2109.05771},
	doi = {10.48550/arXiv.2109.05771},
	abstract = {Natural Language Generation (NLG) evaluation is a multifaceted task requiring assessment of multiple desirable criteria, e.g., fluency, coherency, coverage, relevance, adequacy, overall quality, etc. Across existing datasets for 6 NLG tasks, we observe that the human evaluation scores on these multiple criteria are often not correlated. For example, there is a very low correlation between human scores on fluency and data coverage for the task of structured data to text generation. This suggests that the current recipe of proposing new automatic evaluation metrics for NLG by showing that they correlate well with scores assigned by humans for a single criteria (overall quality) alone is inadequate. Indeed, our extensive study involving 25 automatic evaluation metrics across 6 different tasks and 18 different evaluation criteria shows that there is no single metric which correlates well with human scores on all desirable criteria, for most NLG tasks. Given this situation, we propose CheckLists for better design and evaluation of automatic metrics. We design templates which target a specific criteria (e.g., coverage) and perturb the output such that the quality gets affected only along this specific criteria (e.g., the coverage drops). We show that existing evaluation metrics are not robust against even such simple perturbations and disagree with scores assigned by humans to the perturbed output. The proposed templates thus allow for a fine-grained assessment of automatic evaluation metrics exposing their limitations and will facilitate better design, analysis and evaluation of such metrics.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Sai, Ananya B. and Dixit, Tanay and Sheth, Dev Yashpal and Mohan, Sreyas and Khapra, Mitesh M.},
	month = sep,
	year = {2021},
	note = {arXiv:2109.05771 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{sai_survey_2022,
	title = {A {Survey} of {Evaluation} {Metrics} {Used} for {NLG} {Systems}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3485766},
	doi = {10.1145/3485766},
	abstract = {In the last few years, a large number of automatic evaluation metrics have been proposed for evaluating Natural Language Generation (NLG) systems. The rapid development and adoption of such automatic evaluation metrics in a relatively short time has created the need for a survey of these metrics. In this survey, we (i) highlight the challenges in automatically evaluating NLG systems, (ii) propose a coherent taxonomy for organising existing evaluation metrics, (iii) briefly describe different existing metrics, and finally (iv) discuss studies criticising the use of automatic evaluation metrics. We then conclude the article highlighting promising future directions of research.},
	number = {2},
	urldate = {2024-05-08},
	journal = {ACM Computing Surveys},
	author = {Sai, Ananya B. and Mohankumar, Akash Kumar and Khapra, Mitesh M.},
	month = jan,
	year = {2022},
	keywords = {Automatic evaluation metrics, abstractive summarization, correlations, data-to-text generation, image captioning, question answering, question generation},
	pages = {26:1--26:39},
}

@inproceedings{frisoni_nlg-metricverse_2022,
	address = {Gyeongju, Republic of Korea},
	title = {{NLG}-{Metricverse}: {An} {End}-to-{End} {Library} for {Evaluating} {Natural} {Language} {Generation}},
	shorttitle = {{NLG}-{Metricverse}},
	url = {https://aclanthology.org/2022.coling-1.306},
	abstract = {Driven by deep learning breakthroughs, natural language generation (NLG) models have been at the center of steady progress in the last few years, with a ubiquitous task influence. However, since our ability to generate human-indistinguishable artificial text lags behind our capacity to assess it, it is paramount to develop and apply even better automatic evaluation metrics. To facilitate researchers to judge the effectiveness of their models broadly, we introduce NLG-Metricverse—an end-to-end open-source library for NLG evaluation based on Python. Our framework provides a living collection of NLG metrics in a unified and easy-to-use environment, supplying tools to efficiently apply, analyze, compare, and visualize them. This includes (i) the extensive support to heterogeneous automatic metrics with n-arity management, (ii) the meta-evaluation upon individual performance, metric-metric and metric-human correlations, (iii) graphical interpretations for helping humans better gain score intuitions, (iv) formal categorization and convenient documentation to accelerate metrics understanding. NLG-Metricverse aims to increase the comparability and replicability of NLG research, hopefully stimulating new contributions in the area.},
	urldate = {2024-05-08},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Frisoni, Giacomo and Carbonaro, Antonella and Moro, Gianluca and Zammarchi, Andrea and Avagnano, Marco},
	editor = {Calzolari, Nicoletta and Huang, Chu-Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key-Sun and Ryu, Pum-Mo and Chen, Hsin-Hsi and Donatelli, Lucia and Ji, Heng and Kurohashi, Sadao and Paggio, Patrizia and Xue, Nianwen and Kim, Seokhwan and Hahm, Younggyun and He, Zhong and Lee, Tony Kyungil and Santus, Enrico and Bond, Francis and Na, Seung-Hoon},
	month = oct,
	year = {2022},
	pages = {3465--3479},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-03-25},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {4171--4186},
}

@misc{welleck_neural_2019,
	title = {Neural {Text} {Generation} with {Unlikelihood} {Training}},
	url = {http://arxiv.org/abs/1908.04319},
	doi = {10.48550/arXiv.1908.04319},
	abstract = {Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-\$k\$ and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason},
	month = sep,
	year = {2019},
	note = {arXiv:1908.04319 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{welleck_consistency_2020,
	title = {Consistency of a {Recurrent} {Language} {Model} {With} {Respect} to {Incomplete} {Decoding}},
	url = {http://arxiv.org/abs/2002.02492},
	doi = {10.48550/arXiv.2002.02492},
	abstract = {Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms. To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model. We prove that commonly used incomplete decoding algorithms - greedy search, beam search, top-k sampling, and nucleus sampling - are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length. Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model. Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency.},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Welleck, Sean and Kulikov, Ilia and Kim, Jaedeok and Pang, Richard Yuanzhe and Cho, Kyunghyun},
	month = oct,
	year = {2020},
	note = {arXiv:2002.02492 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{chung_unimax_2022,
	title = {{UniMax}: {Fairer} and {More} {Effective} {Language} {Sampling} for {Large}-{Scale} {Multilingual} {Pretraining}},
	shorttitle = {{UniMax}},
	url = {https://openreview.net/forum?id=kXwdL1cWOAi},
	abstract = {Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with UniMax sampling.},
	language = {en},
	urldate = {2024-03-25},
	author = {Chung, Hyung Won and Garcia, Xavier and Roberts, Adam and Tay, Yi and Firat, Orhan and Narang, Sharan and Constant, Noah},
	month = sep,
	year = {2022},
}

@inproceedings{wang_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT} has a {Mouth}, and {It} {Must} {Speak}: {BERT} as a {Markov} {Random} {Field} {Language} {Model}},
	shorttitle = {{BERT} has a {Mouth}, and {It} {Must} {Speak}},
	url = {https://aclanthology.org/W19-2304},
	doi = {10.18653/v1/W19-2304},
	abstract = {We show that BERT (Devlin et al., 2018) is a Markov random field language model. This formulation gives way to a natural procedure to sample sentences from BERT. We generate from BERT and find that it can produce high quality, fluent generations. Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.},
	urldate = {2024-03-25},
	booktitle = {Proceedings of the {Workshop} on {Methods} for {Optimizing} and {Evaluating} {Neural} {Language} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Alex and Cho, Kyunghyun},
	editor = {Bosselut, Antoine and Celikyilmaz, Asli and Ghazvininejad, Marjan and Iyer, Srinivasan and Khandelwal, Urvashi and Rashkin, Hannah and Wolf, Thomas},
	month = jun,
	year = {2019},
	pages = {30--36},
}

@misc{ancona_towards_2018,
	title = {Towards better understanding of gradient-based attribution methods for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.06104},
	doi = {10.48550/arXiv.1711.06104},
	abstract = {Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
	month = mar,
	year = {2018},
	note = {arXiv:1711.06104 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{sikdar_integrated_2021,
	address = {Online},
	title = {Integrated {Directional} {Gradients}: {Feature} {Interaction} {Attribution} for {Neural} {NLP} {Models}},
	shorttitle = {Integrated {Directional} {Gradients}},
	url = {https://aclanthology.org/2021.acl-long.71},
	doi = {10.18653/v1/2021.acl-long.71},
	abstract = {In this paper, we introduce Integrated Directional Gradients (IDG), a method for attributing importance scores to groups of features, indicating their relevance to the output of a neural network model for a given input. The success of Deep Neural Networks has been attributed to their ability to capture higher level feature interactions. Hence, in the last few years capturing the importance of these feature interactions has received increased prominence in ML interpretability literature. In this paper, we formally define the feature group attribution problem and outline a set of axioms that any intuitive feature group attribution method should satisfy. Earlier, cooperative game theory inspired axiomatic methods only borrowed axioms from solution concepts (such as Shapley value) for individual feature attributions and introduced their own extensions to model interactions. In contrast, our formulation is inspired by axioms satisfied by characteristic functions as well as solution concepts in cooperative game theory literature. We believe that characteristic functions are much better suited to model importance of groups compared to just solution concepts. We demonstrate that our proposed method, IDG, satisfies all the axioms. Using IDG we analyze two state-of-the-art text classifiers on three benchmark datasets for sentiment analysis. Our experiments show that IDG is able to effectively capture semantic interactions in linguistic models via negations and conjunctions.},
	urldate = {2024-03-25},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sikdar, Sandipan and Bhattacharya, Parantapa and Heese, Kieran},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {865--878},
}

@inproceedings{ross_explaining_2021,
	address = {Online},
	title = {Explaining {NLP} {Models} via {Minimal} {Contrastive} {Editing} ({MiCE})},
	url = {https://aclanthology.org/2021.findings-acl.336},
	doi = {10.18653/v1/2021.findings-acl.336},
	urldate = {2024-03-25},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Ross, Alexis and Marasović, Ana and Peters, Matthew},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {3840--3852},
}

@misc{raffel_exploring_2023-1,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	doi = {10.48550/arXiv.1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = sep,
	year = {2023},
	note = {arXiv:1910.10683 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{xue_byt5_2022,
	title = {{ByT5}: {Towards} a {Token}-{Free} {Future} with {Pre}-trained {Byte}-to-{Byte} {Models}},
	volume = {10},
	shorttitle = {{ByT5}},
	url = {https://aclanthology.org/2022.tacl-1.17},
	doi = {10.1162/tacl_a_00461},
	abstract = {Most widely used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: They can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Because byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.1},
	urldate = {2024-03-25},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Xue, Linting and Barua, Aditya and Constant, Noah and Al-Rfou, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
	editor = {Roark, Brian and Nenkova, Ani},
	year = {2022},
	pages = {291--306},
}

@inproceedings{xue_mt5_2021,
	address = {Online},
	title = {{mT5}: {A} {Massively} {Multilingual} {Pre}-trained {Text}-to-{Text} {Transformer}},
	shorttitle = {{mT5}},
	url = {https://aclanthology.org/2021.naacl-main.41},
	doi = {10.18653/v1/2021.naacl-main.41},
	abstract = {The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent “accidental translation” in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.},
	urldate = {2024-03-25},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {483--498},
}
